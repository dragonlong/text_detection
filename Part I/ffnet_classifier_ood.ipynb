{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:29:26.331001Z",
     "start_time": "2018-01-17T15:29:25.522327Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import python library\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "# configure and test\n",
    "tf.test.gpu_device_name()\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2,3],[1, 2,3]], dtype = tf.float32)\n",
    "a.get_shape().as_list()[1]\n",
    "shape = [5, 5, 3, 32]\n",
    "print(shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     79
    ]
   },
   "outputs": [],
   "source": [
    "# define the neural network architecture \n",
    "# define needed variables during training \n",
    "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
    "def _variable_summaries(var):\n",
    "    \"\"\"\n",
    "    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    \"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "    \n",
    "class model(object):\n",
    "    # class level attributes, used for \n",
    "    number = 0\n",
    "    def __init__(self, name):\n",
    "        model.name = name\n",
    "    # finally build our model according to our own need\n",
    "    def builder(self, x, y):\n",
    "        self.conv1 = self._conv_layer(x,     [5, 5, 3, 32],  'conv1')\n",
    "        self.conv2 = self._conv_layer(self.conv1, [3, 3, 32, 64], 'conv2')\n",
    "        self.pool1 = self._max_pool22(self.conv2)\n",
    "        # ffnet1\n",
    "        self.ffn1  = self._ffnet_module(self.pool1,f = [3, 5], filters = [64, 64, 64, 64] , stage = 1, block = 'a')\n",
    "        self.ffn2  = self._ffnet_module(self.ffn1 ,f = [3, 5], filters = [64, 64, 64, 64],  stage = 2, block = 'a' )\n",
    "        self.fc1   = self._fc_layer(tf.reshape(self.ffn1, [-1, 8*8*128]), 8*8*128, 256, 'fc1' )\n",
    "        self.fc2   = self._fc_layer(self.fc1, 256, 10, 'fc2')\n",
    "        self.y_    = tf.nn.softmax(self._fc_layer(self.fc2, 10, 2, 'fc2'), name='output')\n",
    "    # class methods to construct conv-layer, generating summary autotically \n",
    "    # with default stride 1, and relu activations\n",
    "    def _conv_layer(self, X, shape, layer_name, padding='SAME'):\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('W'):\n",
    "                weight = self.get_filters(shape)\n",
    "                _variable_summaries(weight)\n",
    "            with tf.name_scope('b'):\n",
    "                print(shape[3])\n",
    "                bias = self.get_bias([shape[3]])\n",
    "                print(shape[3])\n",
    "                _variable_summaries(bias)\n",
    "            with tf.name_scope('z'):\n",
    "                preactivations = tf.nn.conv2d(X, weight, strides=[1, 1, 1, 1], padding=padding) + bias\n",
    "                _variable_summaries(preactivations)\n",
    "            with tf.name_scope('A'):\n",
    "                activations    = tf.nn.relu(preactivations)\n",
    "                _variable_summaries(activations)\n",
    "        return activations\n",
    "    \n",
    "    # ex: prob = tf.constant(0.5, dtype=tf.float32)\n",
    "    def _drop_out(self, X, prob, layer_name):\n",
    "        with tf.name_scope(layer_name+'Dropout'):\n",
    "            return tf.nn.dropout(X, prob)\n",
    "    def _max_pool22(self, X, padding = 'SAME'):\n",
    "        return tf.nn.max_pool(X, strides= [1, 2, 2, 1], ksize=[1, 2, 2, 1], padding= padding)\n",
    "           \n",
    "    def _fc_layer(self, X, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "      # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('W'):\n",
    "                weights = self.get_weights([input_dim, output_dim])\n",
    "                _variable_summaries(weights)\n",
    "            with tf.name_scope('b'):\n",
    "                biases = self.get_bias([output_dim])\n",
    "                _variable_summaries(biases)\n",
    "            with tf.name_scope('z'):\n",
    "                preactivate = tf.matmul(X, weights) + biases\n",
    "                tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "\n",
    "    def _ffnet_module(self, X, f, filters, stage, block):\n",
    "        \"\"\"\n",
    "        Implementation of the figure above, with 3 standard 3*3*64 module for the general module, and one fast-forwarding path\n",
    "        Arguments: \n",
    "        X--      the input tensor with shape (n_H, n_W, n_C)\n",
    "        f--      filter kernel size\n",
    "        filters: number of filters in each layer\n",
    "        stage  : name of stage \n",
    "        block  : string/character, used to name the layers, depending on \n",
    "\n",
    "        Returns:\n",
    "        X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "        \"\"\"\n",
    "        conv_name_base = 'ffnet' + str(stage) + block + 'conv_branch'\n",
    "        bn_name_base   = 'bn' + str(stage) + block + 'bn_branch'\n",
    "        fast_fwd_base  = 'ffnet' + str(stage) + block + 'fast_branch' \n",
    "\n",
    "        # Retrieve filters \n",
    "        F1, F2, F3, F4 = filters\n",
    "        f1, f2     = f\n",
    "        m = X.get_shape().as_list()[3]\n",
    "\n",
    "        # Save the input value\n",
    "        X_fast = X\n",
    "\n",
    "        ####### Main Path ######\n",
    "        X = self._conv_layer(X, [f1, f1, m, F1],  layer_name = conv_name_base + '1', padding = 'VALID')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_1', X)\n",
    "        X = self._conv_layer(X, [f1, f1, F1, F2], layer_name = conv_name_base + '2', padding = 'VALID')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_2', X)\n",
    "        X = self._conv_layer(X, [f1, f1, F2, F3], layer_name = conv_name_base + '3', padding = 'SAME')\n",
    "        ####### fast forward ###### \n",
    "        X_fast = self._conv_layer(X_fast, [f2, f2, m, F4], layer_name = fast_fwd_base + '1', padding = 'VALID')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_4', X_fast)\n",
    "        print(X_fast)\n",
    "        print(X)\n",
    "        result = tf.concat([X, X_fast], 3) \n",
    "        print(result)\n",
    "        ###### Final step: concatation #######\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def get_filters(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return(tf.Variable(initial))\n",
    "    \n",
    "    def get_weights(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return(tf.Variable(initial))\n",
    "    \n",
    "    def get_bias(self, output_dim):\n",
    "        initial = tf.constant(0.1, shape = output_dim)\n",
    "        return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# encoding the labels information\n",
    "def label_encoder(labels, C, style = 'one_hot_matrix'):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    encoded labels in a pattern of matrix\n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if style is 'one_hot_matrix':\n",
    "        # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "        C = tf.constant(C, name='C')\n",
    "\n",
    "        # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "        one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n",
    "\n",
    "        # Create the session (approx. 1 line)\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Run the session (approx. 1 line)\n",
    "        label_encoded = sess.run(one_hot_matrix)\n",
    "\n",
    "        # Close the session (approx. 1 line). See method 1 above.\n",
    "        sess.close()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Using TFrecorder to store and read data\n",
    "#### 1. List images and their label \n",
    "#### 2. Create a TFRecord file\n",
    "-   Define function to load images and convert data into features\n",
    "-   Open a TFrecord file \n",
    "-   Create a feature and pass the converted data into it\n",
    "-   Create an example protocol\n",
    "-   Serialize the example \n",
    "-   Write the example to TFRecord file\n",
    "\n",
    "#### 3. Read the TFRecord file \n",
    "- Create a list of filenames\n",
    "- Create a queue to hold filenames \n",
    "- Define a reader\n",
    "- Define a decoder \n",
    "- Convert the data from string back to numbers \n",
    "- Reshape data into its original shape\n",
    "- Preprocessing\n",
    "- batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dragonx/Documents/text_detect'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 106748 txt instances, and 123588 nontxt instances\n",
      "with 230336 images in total\n",
      "divided into train: 161235 , val: 46067 test: 23033  images\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "import glob\n",
    "shuffle_data  = True # whether we will need to shuffle the filenames \n",
    "txt_patch1    = './dataC/COCO-Text-Patch/images/txt/*.jpg'\n",
    "txt_patch2    = './dataA/COCO-Text-Patch/images/txt/*.jpg'\n",
    "nontxt_patch1 = './dataC/COCO-Text-Patch/images/nontxt/*.jpg'\n",
    "nontxt_patch2 = './dataA/COCO-Text-Patch/images/nontxt/*.jpg'\n",
    "addrs_txt  = glob.glob(txt_patch1) + glob.glob(txt_patch2)\n",
    "labels_txt = [0 for addr in addrs_txt]\n",
    "addrs_nontxt  = glob.glob(nontxt_patch1) + glob.glob(nontxt_patch2)\n",
    "labels_nontxt = [1 for addr in addrs_nontxt]\n",
    "labels = labels_txt + labels_nontxt\n",
    "addrs  = addrs_txt + addrs_nontxt\n",
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)\n",
    "# test module assert len(labels_txt)\n",
    "print('we have %d txt instances, and %d nontxt instances'%(len(labels_txt),len(labels_nontxt)))\n",
    "print('with %d images in total'%(len(labels)))\n",
    "# print(','.join([str(x) for x in labels[0:100]]))\n",
    "# print('[%s]'%', '.join(map(str, labels[0:100])))\n",
    "# print(' '.join(['{:01d}'.format(x) for x in labels[0:100]]))\n",
    "# divide dataset into train, dev, test with 70 20 10%\n",
    "train_addrs = addrs[0:int(0.7*len(addrs))]\n",
    "train_labels = labels[0:int(0.7* len(labels))]\n",
    "\n",
    "dev_addrs = addrs[int(0.7*len(addrs)):int(0.9*len(addrs))]\n",
    "dev_labels = labels[int(0.7*len(labels)):int(0.9*len(labels))]\n",
    "\n",
    "test_addrs = addrs[int(0.9*len(addrs)):len(addrs)]\n",
    "test_labels = labels[int(0.9*len(labels)):len(labels)]\n",
    "print('divided into train: %d , val: %d test: %d  images'%(int(0.7*len(addrs)), int(0.2*len(addrs)), int(0.1*len(addrs)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/161235\n",
      "Train data: 1000/161235\n",
      "Train data: 2000/161235\n",
      "Train data: 3000/161235\n",
      "Train data: 4000/161235\n",
      "Train data: 5000/161235\n",
      "Train data: 6000/161235\n",
      "Train data: 7000/161235\n",
      "Train data: 8000/161235\n",
      "Train data: 9000/161235\n",
      "Train data: 10000/161235\n",
      "Train data: 11000/161235\n",
      "Train data: 12000/161235\n",
      "Train data: 13000/161235\n",
      "Train data: 14000/161235\n",
      "Train data: 15000/161235\n",
      "Train data: 16000/161235\n",
      "Train data: 17000/161235\n",
      "Train data: 18000/161235\n",
      "Train data: 19000/161235\n",
      "Train data: 20000/161235\n",
      "Train data: 21000/161235\n",
      "Train data: 22000/161235\n",
      "Train data: 23000/161235\n",
      "Train data: 24000/161235\n",
      "Train data: 25000/161235\n",
      "Train data: 26000/161235\n",
      "Train data: 27000/161235\n",
      "Train data: 28000/161235\n",
      "Train data: 29000/161235\n",
      "Train data: 30000/161235\n",
      "Train data: 31000/161235\n",
      "Train data: 32000/161235\n",
      "Train data: 33000/161235\n",
      "Train data: 34000/161235\n",
      "Train data: 35000/161235\n",
      "Train data: 36000/161235\n",
      "Train data: 37000/161235\n",
      "Train data: 38000/161235\n",
      "Train data: 39000/161235\n",
      "Train data: 40000/161235\n",
      "Train data: 41000/161235\n",
      "Train data: 42000/161235\n",
      "Train data: 43000/161235\n",
      "Train data: 44000/161235\n",
      "Train data: 45000/161235\n",
      "Train data: 46000/161235\n",
      "Train data: 47000/161235\n",
      "Train data: 48000/161235\n",
      "Train data: 49000/161235\n",
      "Train data: 50000/161235\n",
      "Train data: 51000/161235\n",
      "Train data: 52000/161235\n",
      "Train data: 53000/161235\n",
      "Train data: 54000/161235\n",
      "Train data: 55000/161235\n",
      "Train data: 56000/161235\n",
      "Train data: 57000/161235\n",
      "Train data: 58000/161235\n",
      "Train data: 59000/161235\n",
      "Train data: 60000/161235\n",
      "Train data: 61000/161235\n",
      "Train data: 62000/161235\n",
      "Train data: 63000/161235\n",
      "Train data: 64000/161235\n",
      "Train data: 65000/161235\n",
      "Train data: 66000/161235\n",
      "Train data: 67000/161235\n",
      "Train data: 68000/161235\n",
      "Train data: 69000/161235\n",
      "Train data: 70000/161235\n",
      "Train data: 71000/161235\n",
      "Train data: 72000/161235\n",
      "Train data: 73000/161235\n",
      "Train data: 74000/161235\n",
      "Train data: 75000/161235\n",
      "Train data: 76000/161235\n",
      "Train data: 77000/161235\n",
      "Train data: 78000/161235\n",
      "Train data: 79000/161235\n",
      "Train data: 80000/161235\n",
      "Train data: 81000/161235\n",
      "Train data: 82000/161235\n",
      "Train data: 83000/161235\n",
      "Train data: 84000/161235\n",
      "Train data: 85000/161235\n",
      "Train data: 86000/161235\n",
      "Train data: 87000/161235\n",
      "Train data: 88000/161235\n",
      "Train data: 89000/161235\n",
      "Train data: 90000/161235\n",
      "Train data: 91000/161235\n",
      "Train data: 92000/161235\n",
      "Train data: 93000/161235\n",
      "Train data: 94000/161235\n",
      "Train data: 95000/161235\n",
      "Train data: 96000/161235\n",
      "Train data: 97000/161235\n",
      "Train data: 98000/161235\n",
      "Train data: 99000/161235\n",
      "Train data: 100000/161235\n",
      "Train data: 101000/161235\n",
      "Train data: 102000/161235\n",
      "Train data: 103000/161235\n",
      "Train data: 104000/161235\n",
      "Train data: 105000/161235\n",
      "Train data: 106000/161235\n",
      "Train data: 107000/161235\n",
      "Train data: 108000/161235\n",
      "Train data: 109000/161235\n",
      "Train data: 110000/161235\n",
      "Train data: 111000/161235\n",
      "Train data: 112000/161235\n",
      "Train data: 113000/161235\n",
      "Train data: 114000/161235\n",
      "Train data: 115000/161235\n",
      "Train data: 116000/161235\n",
      "Train data: 117000/161235\n",
      "Train data: 118000/161235\n",
      "Train data: 119000/161235\n",
      "Train data: 120000/161235\n",
      "Train data: 121000/161235\n",
      "Train data: 122000/161235\n",
      "Train data: 123000/161235\n",
      "Train data: 124000/161235\n",
      "Train data: 125000/161235\n",
      "Train data: 126000/161235\n",
      "Train data: 127000/161235\n",
      "Train data: 128000/161235\n",
      "Train data: 129000/161235\n",
      "Train data: 130000/161235\n",
      "Train data: 131000/161235\n",
      "Train data: 132000/161235\n",
      "Train data: 133000/161235\n",
      "Train data: 134000/161235\n",
      "Train data: 135000/161235\n",
      "Train data: 136000/161235\n",
      "Train data: 137000/161235\n",
      "Train data: 138000/161235\n",
      "Train data: 139000/161235\n",
      "Train data: 140000/161235\n",
      "Train data: 141000/161235\n",
      "Train data: 142000/161235\n",
      "Train data: 143000/161235\n",
      "Train data: 144000/161235\n",
      "Train data: 145000/161235\n",
      "Train data: 146000/161235\n",
      "Train data: 147000/161235\n",
      "Train data: 148000/161235\n",
      "Train data: 149000/161235\n",
      "Train data: 150000/161235\n",
      "Train data: 151000/161235\n",
      "Train data: 152000/161235\n",
      "Train data: 153000/161235\n",
      "Train data: 154000/161235\n",
      "Train data: 155000/161235\n",
      "Train data: 156000/161235\n",
      "Train data: 157000/161235\n",
      "Train data: 158000/161235\n",
      "Train data: 159000/161235\n",
      "Train data: 160000/161235\n",
      "Train data: 161000/161235\n"
     ]
    }
   ],
   "source": [
    "# function to load images\n",
    "# print\n",
    "# Features \n",
    "import sys\n",
    "def load_image(addr):\n",
    "    # read image and keep the size (32, 32, 3) currently \n",
    "    img = cv2.imread(addr)\n",
    "    #img = cvb2.resize(img, (180, 180), interpolation= cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    return img\n",
    "# function to convert data into features\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value = [value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))\n",
    "\n",
    "###########so now let's start to build our TFRecord object############\n",
    "train_filename = 'train.tfrecords'\n",
    "writer         =tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(train_addrs)):\n",
    "    # saving every 1000 images and print how many batches \n",
    "    if not i%1000:\n",
    "        print('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "        sys.stdout.flush()\n",
    "    # where we load the image\n",
    "    img = load_image(train_addrs[i])\n",
    "    label = train_labels[i]\n",
    "    # now we transform the data into features in TFRecord data, reduce the file size into binary files\n",
    "    feature = {'dataset/label': _int64_feature(label),\n",
    "               'dataset/data' : _bytes_feature(tf.compat.as_bytes(img.tostring())) }\n",
    "    # Example of format to write data into a TFRecord object\n",
    "    example = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "    # finally we wil write the data into file sequentially \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data: 0/46067\n",
      "Val data: 1000/46067\n",
      "Val data: 2000/46067\n",
      "Val data: 3000/46067\n",
      "Val data: 4000/46067\n",
      "Val data: 5000/46067\n",
      "Val data: 6000/46067\n",
      "Val data: 7000/46067\n",
      "Val data: 8000/46067\n",
      "Val data: 9000/46067\n",
      "Val data: 10000/46067\n",
      "Val data: 11000/46067\n",
      "Val data: 12000/46067\n",
      "Val data: 13000/46067\n",
      "Val data: 14000/46067\n",
      "Val data: 15000/46067\n",
      "Val data: 16000/46067\n",
      "Val data: 17000/46067\n",
      "Val data: 18000/46067\n",
      "Val data: 19000/46067\n",
      "Val data: 20000/46067\n",
      "Val data: 21000/46067\n",
      "Val data: 22000/46067\n",
      "Val data: 23000/46067\n",
      "Val data: 24000/46067\n",
      "Val data: 25000/46067\n",
      "Val data: 26000/46067\n",
      "Val data: 27000/46067\n",
      "Val data: 28000/46067\n",
      "Val data: 29000/46067\n",
      "Val data: 30000/46067\n",
      "Val data: 31000/46067\n",
      "Val data: 32000/46067\n",
      "Val data: 33000/46067\n",
      "Val data: 34000/46067\n",
      "Val data: 35000/46067\n",
      "Val data: 36000/46067\n",
      "Val data: 37000/46067\n",
      "Val data: 38000/46067\n",
      "Val data: 39000/46067\n",
      "Val data: 40000/46067\n",
      "Val data: 41000/46067\n",
      "Val data: 42000/46067\n",
      "Val data: 43000/46067\n",
      "Val data: 44000/46067\n",
      "Val data: 45000/46067\n",
      "Val data: 46000/46067\n"
     ]
    }
   ],
   "source": [
    "# Validation data and test data \n",
    "val_filename = 'val.tfrecords'\n",
    "writer       = tf.python_io.TFRecordWriter(val_filename)\n",
    "for i in range(len(dev_addrs)):\n",
    "    # saving every 1000 images and print how many batches \n",
    "    if not i%1000:\n",
    "        print('Val data: {}/{}'.format(i, len(dev_addrs)))\n",
    "        sys.stdout.flush()\n",
    "    # where we load the image\n",
    "    img = load_image(dev_addrs[i])\n",
    "    label = dev_labels[i]\n",
    "    # now we transform the data into features in TFRecord data, reduce the file size into binary files\n",
    "    feature = {'dataset/label': _int64_feature(label),\n",
    "               'dataset/data' : _bytes_feature(tf.compat.as_bytes(img.tostring())) }\n",
    "    # Example of format to write data into a TFRecord object\n",
    "    example = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "    # finally we wil write the data into file sequentially \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data: 0/23034\n",
      "Val data: 1000/23034\n",
      "Val data: 2000/23034\n",
      "Val data: 3000/23034\n",
      "Val data: 4000/23034\n",
      "Val data: 5000/23034\n",
      "Val data: 6000/23034\n",
      "Val data: 7000/23034\n",
      "Val data: 8000/23034\n",
      "Val data: 9000/23034\n",
      "Val data: 10000/23034\n",
      "Val data: 11000/23034\n",
      "Val data: 12000/23034\n",
      "Val data: 13000/23034\n",
      "Val data: 14000/23034\n",
      "Val data: 15000/23034\n",
      "Val data: 16000/23034\n",
      "Val data: 17000/23034\n",
      "Val data: 18000/23034\n",
      "Val data: 19000/23034\n",
      "Val data: 20000/23034\n",
      "Val data: 21000/23034\n",
      "Val data: 22000/23034\n",
      "Val data: 23000/23034\n"
     ]
    }
   ],
   "source": [
    "# test data \n",
    "# filename with s, batch or meta data without s, format without s, looking to the \n",
    "test_filename = 'test.tfrecords'\n",
    "writer       = tf.python_io.TFRecordWriter(test_filename)\n",
    "for i in range(len(test_addrs)):\n",
    "    # saving every 1000 images and print how many batches \n",
    "    if not i%1000:\n",
    "        print('Val data: {}/{}'.format(i, len(test_addrs)))\n",
    "        sys.stdout.flush()\n",
    "    # where we load the image\n",
    "    img   = load_image(test_addrs[i])\n",
    "    label = test_labels[i]\n",
    "    # now we transform the data into features in TFRecord data, reduce the file size into binary files\n",
    "    feature = {'dataset/label': _int64_feature(label),\n",
    "               'dataset/data' : _bytes_feature(tf.compat.as_bytes(img.tostring())) }\n",
    "    # Example of format to write data into a TFRecord object\n",
    "    example = tf.train.Example(features = tf.train.Features(feature = feature))\n",
    "    # finally we wil write the data into file sequentially \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TFRecord file from file path according to the file path \n",
    "# () for multiple output, return \n",
    "data_path = 'train.tfrecords'\n",
    "def tfrecord_read(data_path):\n",
    "    with tf.Session() as sess:\n",
    "        feature = {'dataset/data' : tf.FixedLenFeature([], tf.string),\n",
    "                   'dataset/label': tf.FixedLenFeature([], tf.int64)}\n",
    "        # Create a list of filenames and pass it to a queue\n",
    "        filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n",
    "        # Define a reader and read the next record\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "        # Decode the record read by the reader\n",
    "        features = tf.parse_single_example(serialized_example, features=feature)\n",
    "        # Convert the image data from string back to the numbers\n",
    "        image = tf.decode_raw(features['dataset/data'], tf.float32)\n",
    "\n",
    "        # Cast label data into int32\n",
    "        label = tf.cast(features['dataset/label'], tf.int32)\n",
    "        # Reshape image data into the original shape\n",
    "        image = tf.reshape(image, [32, 32, 3])\n",
    "\n",
    "        # Any preprocessing here ...\n",
    "\n",
    "        # Creates batches by randomly shuffling tensors\n",
    "        images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\n",
    "    return (images, labels)\n",
    "\n",
    "(train_data, train_label) = tfrecord_read(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3) (10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82b42bee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3) (10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82b419ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3) (10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82b40889e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3) (10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82ac70a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3) (10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAE0FJREFUeJzt3X2MZXV9x/H3p7sLmKKBRboZFxAomyg1FuyWYIL/GEjRmGAag5i0XR/SjYotpjZxS1ujibbWRkts2ppNJFClKIgpi2lrVooRmxRYUJ6z7opQHnZBxK1imyry7R/3ILMbZufOzJ378DvvV/LL3HvOufees5+Zz9w5556zqSokSbPvlya9ApKk0bDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMs9BVI8vYk31zC8h9O8vnVXCctLMmDSc4dctlKctpqr5NWzlyfZ6FLUiN6U+jdb/E/TnJXkv9O8sUkR3Xzfj/J3iRPJdmR5GXzHldJ3p1kT5IDSf4uA68EPgO8NsnT3bwjknw7yR90j12T5D+SfCjJ+cClwFu75e+cxL9DXyX5HHAScEP37//BJN9L8pJu/huS7E9yfJJvdA+7s1v2rRNbcR2WuR6iqnoxgAeBW4GXAeuB+4F3A68HngReAxwJ/C3wjXmPK+ArwDEMvnG+D5zfzXs78M1DXudVwA+BVwJ/CvwnsKab92Hg85P+t+jr6L4Hzp13/yrgCuA44DHgTYfkftqk19lhrksZa1f+K2GmfLqqHgNIcgNwBvCbwOVVdUc3/U+AHyY5uaoe7B738ao6ABxIclP3uH97oReoqnuSfBT4Z+BXgLOq6ueruVFatouBu4CvAzdU1Vcmuzoakd7m2ptdLp39827/D3A0g3fsDz03saqeBn4AbFzkcYdzJfBy4F+qas9KVlirp/slfS2Dv6o+OeHV0Yj0Ode+FfoLeYxB+QKQ5JcZ/Kn26BCPXehSlX/PYDfNbyU5Z4jlNR4H/fsnOQN4J3A18OmJrJFGwVw7Fvog9HckOSPJkcBfALfM291yOI8DJyQ54rkJSX4X+A0G+9f/ELgyydHzlj85if/uk/E4cCpAd0D88wwOVL8D2JjkvS+0rKaeuXZ6XyxV9TXgz4HrgH3ArwIXDfnwfwfuBfYneTLJScBlwO9V1dNV9U/ALuBvuuWv7b7+IMkdo9oGDe0vgT9LcgD4X+DhqvqHqvo/4HeAjybZ1C37YQa/jA8kuXAyq6shmWsn3ZFfSdKM6/07dElqhYUuSY2w0CWpESsq9CTnJ9ndnTa/bVQrpcky13aZbduWfVA0yRrgO8B5wCPAbcDbquq+0a2exs1c22W27VvJqf9nAXur6gGAJF8ALgAW/OZI4kdqpkRVZYFZ5jrDDpMrLDFbc50qT1bV8YsttJJdLhuBh+fdf4SDT5cHIMnWJLuS7FrBa2l8zLVdi2ZrrlProcUXWdk79KFU1XZgO/gbvyXm2iZznW0reYf+KHDivPsnMNz1TzTdzLVdZtu4lRT6bcCmJKd01zK5CNgxmtXSBJlru8y2ccve5VJVzyR5H/BVYA2Da4rfO7I100SYa7vMtn1jvZaL++SmxyKfhlgSc50e5tqs26tq82ILeaaoJDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqxKKFnuTyJE8kuWfetPVJdibZ0309dnVXU6Nmru0y2/4a5h36FcD5h0zbBtxYVZuAG7v7mi1XYK6tugKz7aeqWnQAJwP3zLu/G5jrbs8Bu4d8nnJMxzDXNscof2YnvS2Og8auYX4W17I8G6pqX3d7P7BhoQWTbAW2LvN1NF7m2q6hsjXX2bbcQv+FqqokdZj524HtAIdbTtPFXNt1uGzNdbYt91MujyeZA+i+PjG6VdIEmWu7zLYHllvoO4At3e0twPWjWR1NmLm2y2z7YIgDI1cD+4CfAY8A7wKOY3CkfA/wNWC9B89mbphrg2OUP7OT3hbHQWOog6LpghsL98lNj6rKqJ7LXKeHuTbr9qravNhCnikqSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1IhFCz3JiUluSnJfknuTXNJNX59kZ5I93ddjV391NSrm2iZz7bdh3qE/A3ygqk4HzgYuTnI6sA24sao2ATd29zU7zLVN5tpnVbWkAVwPnAfsBua6aXPA7iEeW47pGOba5jDXZseuYfp5SfvQk5wMnAncAmyoqn3drP3AhqU8l6aHubbJXPtn7bALJjkauA54f1X9KMkv5lVVJakFHrcV2LrSFdXqMNc2mWtPDbmbZR3wVeCP5k3zT7gZHuba5jDXZsdodrlk8Kv9s8D9VfWpebN2AFu621sY7KvTjDDXNplrv6X7TbzwAsk5wM3A3cCz3eRLGeyXuwY4CXgIuLCqnlrkuQ7/Yhqn12GuLTLXNt1eVZsXW2jRQh8lv0GmR1Vl8aWGY67Tw1ybNVShe6aoJDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqxKKFnuSoJLcmuTPJvUk+0k0/JcktSfYm+WKSI1Z/dTUq5tomc+25qjrsAAIc3d1eB9wCnA1cA1zUTf8M8J4hnqscUzPMtc1hrm2OXYvlVVWLv0Ovgae7u+u6UcDrgS91068E3rzYc2l6mGubzLXfhtqHnmRNkm8DTwA7ge8CB6rqmW6RR4CNq7OKWi3m2iZz7a+hCr2qfl5VZwAnAGcBrxj2BZJsTbIrya5lrqNWibm2yVz7a0mfcqmqA8BNwGuBY5Ks7WadADy6wGO2V9Xmqtq8ojXVqjHXNplr/wzzKZfjkxzT3X4RcB5wP4NvlLd0i20Brl+tldTomWubzLXnhjjS/WrgW8BdwD3Ah7rppwK3AnuBa4EjPWo+U8Nc2xzm2uYY6lMu6YIbiyTfB34CPDm2F50eL2V6tvvlVXX8qJ6sy/Uhpmsbx2WattlcR2fatnmobMda6ABJdvVx/1wftrsP23ioPmxzH7bxULO6zZ76L0mNsNAlqRGTKPTtE3jNadCH7e7DNh6qD9vch2081Exu89j3oUuSVoe7XF5AkgeTnDvkspXktNVeJ62cubbJXJ9noUtSI8Za6EnOT7K7uybztnG+9rCSfA44CbghydNJPpjke0le0s1/Q5L93Rl53+gedme37FuTnJjkpiT3ddejvqR73PokO5Ps6b4eO6FNHLk+5Not06tszXUGcx3m7KNRDGANg6u+nQocAdwJnD6u11/iuj4InDvv/lXAFcBxwGPAmw45m+60effngNd0t18MfAc4HfgEsK2bvg34q0lvp7kOn2vfsjXX2cx1nO/QzwL2VtUDVfVT4AvABWN8/ZW4mMH1pL8O3FBVX1lowaraV1V3dLd/zOA6GhsZbOuV3WItXY+6F7lC77I11xnMdZyFvhF4eN79mbkmcw2uWnct8Crgk8M+LsnJwJkM/teYDVW1r5u1H9gw2rWcmN7lCr3I1lxnMFcPir6wgz7LmeQM4J3A1cCnh3mCJEcD1wHvr6ofHfTkg7/h/Lzo+K041+5xZjtdzLUzzkJ/FDhx3v0Fr8k8BR5nsO+QJEcBnwcuBd4BbEzy3hda9jlJ1jH4xriqqr783HJJ5rr5cwz+N5kW9CbX7nF9ydZcZzHXMR64WAs8AJzC8wdZfm3SBxEWWNcLgP8CDjD4rfyv8+b9OvAUsKm7/25gX7fshQz+k95/BC475Dn/moMPsHxi0ttprsPn2k3rTbbmOpu5jvvyuW8ELmNwBP3yqvrY2F58TJKcA9wM3A08202+lME+uWsYfMTqIQbfTE9NZCVHrA+5Qv+yNdfZy9VT/yWpER4UlaRGrKjQZ+FMMi2dubbLbNu27F0uSdYwOKPqPAafUb0NeFtV3Te61dO4mWu7zLZ9a1fw2F+cSQaQ5LkzyRb85kjiDvspUVVZYJa5zrDD5ApLzNZcp8qTNcT/KbqSXS4zeyaZDstc22W2s+uhYRZayTv0oSTZCmxd7dfReJlrm8x1tq2k0Ic6k6yqttP9d07+CTcTzLVdi2ZrrrNtJbtcbgM2JTklyRHARcCO0ayWJshc22W2jVv2O/SqeibJ+4Cv8vyZZPeObM00EebaLrNt37hP/fdPuCmxyKchlsRcp4e5Nuv2qtq82EKeKSpJjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhqxaKEnuTzJE0numTdtfZKdSfZ0X49d3dXUqJlru8y2v4Z5h34FcP4h07YBN1bVJuDG7r5myxWYa6uuwGz7qaoWHcDJwD3z7u8G5rrbc8DuIZ+nHNMxzLXNMcqf2Ulvi+OgsWuYn8Xl7kPfUFX7utv7gQ3LfB5NF3Ntl9n2wNqVPkFVVZJaaH6SrcDWlb6Oxstc23W4bM11ti33HfrjSeYAuq9PLLRgVW2vqs1VtXmZr6XxMdd2DZWtuc625Rb6DmBLd3sLcP1oVkcTZq7tMts+GOLAyNXAPuBnwCPAu4DjGBwp3wN8DVjvwbOZG+ba4Bjlz+ykt8Vx0BjqoGi64MbicPtkNV5VlVE9l7lOD3Nt1u3D7AbzTFFJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSIRQs9yYlJbkpyX5J7k1zSTV+fZGeSPd3XY1d/dTUq5tomc+23Yd6hPwN8oKpOB84GLk5yOrANuLGqNgE3dvc1O8y1TebaZ1W1pAFcD5wH7AbmumlzwO4hHluO6Rjm2uYw12bHrmH6eS1LkORk4EzgFmBDVe3rZu0HNizwmK3A1qW8jsbLXNtkrj20hHfmRwO3A7/d3T9wyPwf+ht/doa5tjnMtdkx1Dv0oT7lkmQdcB1wVVV9uZv8eJK5bv4c8MQwz6XpYa5tMtf+GuZTLgE+C9xfVZ+aN2sHsKW7vYXBvjrNCHNtk7n2W7o/rRZeIDkHuBm4G3i2m3wpg/1y1wAnAQ8BF1bVU4s81+FfTOP0Osy1RebapturavNiCy1a6KPkN8j0qKqM6rnMdXqYa7OGKnTPFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY1YtNCTHJXk1iR3Jrk3yUe66ackuSXJ3iRfTHLE6q+uRsVc22SuPVdVhx1AgKO72+uAW4CzgWuAi7rpnwHeM8RzlWNqhrm2Ocy1zbFrsbyqavF36DXwdHd3XTcKeD3wpW76lcCbF3suTQ9zbZO59ttQ+9CTrEnybeAJYCfwXeBAVT3TLfIIsHGBx25NsivJrlGssEbHXNtkrv01VKFX1c+r6gzgBOAs4BXDvkBVba+qzVW1eZnrqFVirm0y1/5a0qdcquoAcBPwWuCYJGu7WScAj4543TQm5tomc+2fYT7lcnySY7rbLwLOA+5n8I3ylm6xLcD1q7WSGj1zbZO59twQR7pfDXwLuAu4B/hQN/1U4FZgL3AtcKRHzWdqmGubw1zbHEN9yiVdcGOR5PvAT4Anx/ai0+OlTM92v7yqjh/Vk3W5PsR0beO4TNM2m+voTNs2D5XtWAsdIMmuPh5w6cN292EbD9WHbe7DNh5qVrfZU/8lqREWuiQ1YhKFvn0CrzkN+rDdfdjGQ/Vhm/uwjYeayW0e+z50SdLqcJeLJDVirIWe5Pwku7tLeG4b52uPS5ITk9yU5L7u8qWXdNPXJ9mZZE/39dhJr+uo9CFX6F+25jp7uY5tl0uSNcB3GJy59ghwG/C2qrpvLCswJknmgLmquiPJi4HbGVzZ7u3AU1X18e6H49iq+uAEV3Uk+pIr9Ctbc53NXMf5Dv0sYG9VPVBVPwW+AFwwxtcfi6raV1V3dLd/zOC0640MtvXKbrGWLl/ai1yhd9ma6wzmOs5C3wg8PO/+gpfwbEWSk4EzGfwnAxuqal83az+wYUKrNWq9yxV6ka25zmCuHhRdJUmOBq4D3l9VP5o/rwb7ufx40Ywy2za1kOs4C/1R4MR595u9hGeSdQy+Ma6qqi93kx/v9tU9t8/uiUmt34j1JlfoVbbmOoO5jrPQbwM2df9Z7RHARcCOMb7+WCQJ8Fng/qr61LxZOxhcthTaunxpL3KF3mVrrjOY67ivtvhG4DJgDXB5VX1sbC8+JknOAW4G7gae7SZfymCf3DXASQyuYHdhVT01kZUcsT7kCv3L1lxnL1fPFJWkRnhQVJIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSI/wfoxyILgbuehwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82ac754c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############It's time for us to read data from TFRecords file############\n",
    "# import data in the format of tf.\n",
    "# features and feature \n",
    "# be careful with protocol\n",
    "# debug with whole your heart and whole your goal\n",
    "# image decode shape. what's the input and format\n",
    "height   = tf.constant(32, dtype=tf.int32)\n",
    "width    = tf.constant(32, dtype=tf.int32)\n",
    "def tfrecord_read(filename, num_epochs, batch_size):\n",
    "    feature = { 'dataset/label': tf.FixedLenFeature([], tf.int64),\n",
    "                'dataset/data' : tf.FixedLenFeature([], tf.string)\n",
    "              }\n",
    "    # create a queue to hold filenames \n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      features = feature)\n",
    "    # Decode the jpeg\n",
    "#     with tf.name_scope('decode_jpeg',[image_buffer], None):\n",
    "#         # decode\n",
    "#         image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "#         # resize\n",
    "#         image = tf.image.resize_images(image, [height, width])\n",
    "#         # convert to single precision data type\n",
    "#         image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.decode_raw(features['dataset/data'], tf.float32)\n",
    "    image = tf.reshape(image, [32, 32, 3])\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    label = tf.cast(features['dataset/label'], tf.int32)  \n",
    "    # Creates batches by randomly shuffling tensors\n",
    "    images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=30, num_threads=1, min_after_dequeue=1)\n",
    "    #print(images.shape)\n",
    "    return (images, labels)                               \n",
    "\n",
    "# a general demo module to test our methods\n",
    "%matplotlib inline   \n",
    "filename = 'train.tfrecords'\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "with tf.Session() as sess:\n",
    "    images, labels =  tfrecord_read(filename, 1, 10)\n",
    "    ################# debug ################\n",
    "    ##########log: successfully debug#######\n",
    "     # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord   = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for batch_index in range(5):\n",
    "        #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        #run_metadata = tf.RunMetadata()\n",
    "        img, lbl = sess.run([images, labels])\n",
    "        print(img.shape, lbl.shape)                                   \n",
    "        img      = img.astype(np.uint8)\n",
    "        for j in range(6):\n",
    "            plt.subplot(2, 3, j+1)\n",
    "            plt.imshow(img[j, ...])\n",
    "            plt.title('nontxt' if lbl[j]==0 else 'txt')\n",
    "        plt.show()\n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "# return images, labels\n",
    "# train_data, train_label = tfrecord_read('train.tfrecords', 1, 100)\n",
    "# val_data,   val_label   = tfrecord_read('val.tfrecords',   1, 100)                                           \n",
    "# test_data, test_label   = tfrecord_read('test.tfrecords',  1, 100)\n",
    "# print(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to define the hyper-parametes we use \n",
    "# Finally we got the chance to run our model\n",
    "# Save our model periodly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_input_fn(features, labels, batch_size):\n",
    "#     \"\"\"An input function for training\"\"\"\n",
    "#     # Convert the inputs to a Dataset.\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "#     # Shuffle, repeat, and batch the examples.\n",
    "#     dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "#     # Build the Iterator, and return the read end of the pipeline.\n",
    "#     return dataset.make_one_shot_iterator().get_next()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# explore ways to iomport data: to import data in an iterative style, while aslo assign data and labels dynamically\n",
    "######1. Create Dataset object #######\n",
    "# function for image reading\n",
    "import cv2\n",
    "def read_patch_cv2(filename, label):\n",
    "    image_decoded = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "    print('here I come')\n",
    "    return image_decoded, label\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)#labels are not put here \n",
    "    image = tf.image.decode_image(image_string, channels =  3)\n",
    "    print('Transforming')\n",
    "    return image, label\n",
    "\n",
    "def imgs_input_fn(filenames, labels = None):\n",
    "    # create labels if there is no source\n",
    "    if labels is None:\n",
    "        labels = [0]*len(filenames)\n",
    "    labels = np.array(labels)\n",
    "    # to avoid \n",
    "    if len(labels) == 1:\n",
    "        np.expand_dims(labels, axis = 1)\n",
    "    # convert array/list into tensorflow constants\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "    #abels = tf.cast(labels, tf.float32)\n",
    "    # apply standard tf function mapping images data to dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    return dataset\n",
    "\n",
    "def read_txt(filepath, prefix):\n",
    "    filenames = []\n",
    "    labels= []\n",
    "    # read from text files \n",
    "    with open(filepath, 'r') as f:\n",
    "        num = 0\n",
    "        for line in f:\n",
    "            num +=1 \n",
    "            line = line.strip()\n",
    "            if num < 1:\n",
    "                print('%s'  %(line[:-2]) )\n",
    "                print(line[-1])\n",
    "            filenames.append(prefix+line[:-2])\n",
    "            labels.append(line[-1])\n",
    "    print(num)\n",
    "    return filenames, labels\n",
    "    \n",
    "#print(filenames)\n",
    "filepath_train = '/home/dragonx/Documents/text_detect/dataC/COCO-Text-Patch/train.txt'\n",
    "filepath_val = '/home/dragonx/Documents/text_detect/dataA/COCO-Text-Patch/val.txt'\n",
    "filenames_train, labels_train = read_txt(filepath_train, './dataC/COCO-Text-Patch/images/')\n",
    "filenames_val, labels_val = read_txt(filepath_val, './dataA/COCO-Text-Patch/images/')\n",
    "dataset_train = imgs_input_fn(filenames_train, labels_train)\n",
    "dataset_val   = imgs_input_fn(filenames_val, labels_val)\n",
    "######2. batch_read() ####### \n",
    "#either from file directory, or by the txt file with filenames  \n",
    "\n",
    "######3. Create tfRecord Object #######\n",
    "\n",
    "#(recommended)\n",
    "######4. Read from npy format(one by one or in total)  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Data import with Dataset\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"train-00000-of-00001\"]\n",
    "train = sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "# validation_filenames = [\"validation-00000-of-00001\"]\n",
    "# val = sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108012\n",
      "122324\n",
      "['./dataC/COCO-Text-Patch/images/txt/txt_615639.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_660131.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_235518.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_734094.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_336638.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_374506.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_127162.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_713484.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_43367.jpg']\n"
     ]
    }
   ],
   "source": [
    "# self defined data import as numpy array\n",
    "# data input into filenames, and randomly choose filenames patch by patch, so each time the \n",
    "# data is read from raw data, the time consumes in the \n",
    "import os\n",
    "import cv2\n",
    "train_dir = './dataC/COCO-Text-Patch/images/'\n",
    "val_dir =   './dataA/COCO-Text-Patch/images/'\n",
    "classes = ['txt', 'nontxt']\n",
    "fname_tr   = []\n",
    "labels_tr  = []\n",
    "fname_val  = []\n",
    "labels_val = []\n",
    "for i, c in enumerate(classes):\n",
    "    buffer = os.listdir(train_dir+c)\n",
    "    buffer = [train_dir+c+'/'+x for x in buffer]\n",
    "    fname_tr.extend(buffer[:])\n",
    "    labels_tr.extend([i]*len(buffer))\n",
    "    # the same for val\n",
    "    buffer = os.listdir(val_dir+c)\n",
    "    buffer = [val_dir+c+'/'+x for x in buffer]\n",
    "    fname_val.extend(buffer[:])\n",
    "    labels_val.extend([i]*len(buffer))\n",
    "\n",
    "print(len(fname_tr))\n",
    "print(len(fname_val))\n",
    "# print(fname_tr[0:100])\n",
    "# check if numbers are correct\n",
    "assert len(fname_tr) == len(labels_tr)\n",
    "index_tr = np.arange(0, len(labels_tr))  \n",
    "index_val= np.arange(0, len(labels_val))  \n",
    "def imread_batch(filenames, labels):\n",
    "    train_data = np.zeros([len(filenames), 32, 32, 3])\n",
    "    for i, f in enumerate(filenames):\n",
    "        #print(f)\n",
    "        buffer = cv2.imread(f)\n",
    "        train_data[i, :,:,:] = buffer[:,:,:]\n",
    "        #print(buffer.shape)\n",
    "    labels = label_encoder(labels, C, style = 'one_hot_matrix')\n",
    "    \n",
    "    return train_data, labels\n",
    "\n",
    "#data_tr, labels_tr = imread_batch(fname_tr[0:100], labels_tr[0:100])\n",
    "#data_val,labels_val = imread_batch(fname_val[0:100], labels_val[0:100])\n",
    "# data_tr, labels_tr = imread_batch(fname_tr(index_tr[0:100]) , labels_tr[0:100])\n",
    "print(np.take(fname_tr, index_tr[1:10], axis = 0))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "Tensor(\"ffnet1afast_branch1_4/A/Relu:0\", shape=(100, 12, 12, 64), dtype=float32)\n",
      "Tensor(\"ffnet1aconv_branch3_4/A/Relu:0\", shape=(100, 12, 12, 64), dtype=float32)\n",
      "Tensor(\"concat_16:0\", shape=(100, 12, 12, 128), dtype=float32)\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "Tensor(\"ffnet2afast_branch1_4/A/Relu:0\", shape=(100, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"ffnet2aconv_branch3_4/A/Relu:0\", shape=(100, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"concat_17:0\", shape=(100, 8, 8, 128), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 225 and 100 for 'SoftmaxCrossEntropyWithLogits_4' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [225,2], [100,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 225 and 100 for 'SoftmaxCrossEntropyWithLogits_4' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [225,2], [100,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dc76854ff397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1781\u001b[0m   \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1783\u001b[0;31m       precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m   \u001b[0;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   4362\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4363\u001b[0m         \u001b[0;34m\"SoftmaxCrossEntropyWithLogits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4364\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4365\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4366\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 225 and 100 for 'SoftmaxCrossEntropyWithLogits_4' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [225,2], [100,2]."
     ]
    }
   ],
   "source": [
    "############ for training detail ##############\n",
    "# x, y_: input labels\n",
    "# y_conv: convolutional layers output\n",
    "import time\n",
    "# training hyperparameters, with specifications on saving models\n",
    "batch_size    = 100\n",
    "num_tr        = len(train_addrs)/batch_size\n",
    "epoch_num     = 20\n",
    "train_record  = []\n",
    "global_step   = tf.Variable(10, dtype=tf.int32, trainable=False, name = 'global_step')\n",
    "dir_summaries = 'running_summary'\n",
    "saver         = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # build model \n",
    "    Model = model('ffnet')\n",
    "    x  = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "    y  = tf.placeholder(tf.float32, shape = [None, 2]) \n",
    "    Model.builder(x, y)\n",
    "    Model.loss       = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Model.y_))\n",
    "    Model.optimizer  = tf.train.AdamOptimizer(1e-4).minimize(Model.loss)\n",
    "    Model.correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(Model.y_, 1))\n",
    "    Model.accuracy   = tf.reduce_mean(tf.cast(Model.correct_prediction, tf.float32))\n",
    "    # computational graph for feeding data\n",
    "    train_images, train_labels =  tfrecord_read('train.tfrecords', epoch_num, batch_size)\n",
    "    val_images, val_labels     =  tfrecord_read('val.tfrecords', epoch_num, batch_size)\n",
    "    # summaries for tensorboard \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(dir_summaries + '/train',sess.graph)\n",
    "    val_writer = tf.summary.FileWriter(dir_summaries + '/val')\n",
    "\n",
    "     # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord   = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    # main loop for tensorflow training \n",
    "    for i in range(epoch_num):\n",
    "        start_time = time.time()\n",
    "        print('epoch %d'%(i))\n",
    "        for j in range(np.int64(num_tr)):\n",
    "            t1 = time.time()\n",
    "            # get data using shuffled batch reader from tensorflow \n",
    "            t_img, t_lbl = sess.run([train_images,train_labels])\n",
    "            t_lbl        = label_encoder(t_lbl, 2, style = 'one_hot_matrix').T\n",
    "            v_img, v_lbl = sess.run([val_images, val_labels])\n",
    "            v_lbl        = label_encoder(v_lbl, 2, style = 'one_hot_matrix').T\n",
    "            if (i<1) and (j< 10):\n",
    "                print(t_img.shape)\n",
    "                print(v_img.shape)\n",
    "                import pdb;pdb.set_trace()\n",
    "                print('loading time on %d images is %f seconds'%(batch_size, time.time() -t1))   \n",
    "            # train the network from scratch using GPU, remember, here we use ood\n",
    "            v_img_text = np.random.rand(100, 32, 32, 3)\n",
    "            if (j) % 20 == 0:\n",
    "                summary, val_accuracy = sess.run([merged, Model.accuracy], feed_dict={x: v_img_text, y: v_lbl})\n",
    "                val_writer.add_summary(summary, i*num_tr+j)\n",
    "                train_record.append(val_accuracy)\n",
    "                print('current accuracy on %d images is %f'%(batch_size, val_accuracy))\n",
    "            # validation accuracy\n",
    "            else: \n",
    "                summary, _ = sess.run([merged, Model.optimizer], feed_dict={x: t_img, y: t_lbl})\n",
    "                train_writer.add_summary(summary, i*num_tr+j)\n",
    "        # training time evaluation with timer\n",
    "        # provide checkpoints to save models sequentially\n",
    "        epoch_check = 2\n",
    "        if (i+1)%epoch_check == 0:\n",
    "            # using the savor function\n",
    "            saver.save(sess, 'checkpoint/model_V0', global_step = tf.convert_to_tensor(epoch_check*num_tr))\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary, _ = sess.run([merged, train_step],\n",
    "                                  feed_dict=feed_dict(True),\n",
    "                                  options=run_options,\n",
    "                                  run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, 'step%05d' % i*num_tr)\n",
    "            print('Adding run metadata for', i*num_tr+j)\n",
    "        # data visualization & model summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "    print(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File checkpoint/FFnet_Classify_V0-0.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-569fac543aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint/FFnet_Classify_V0-0.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoint/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#saver.restore(sess, 'checkpoint/FFnet_Classify_')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m                      \"execution is enabled.\")\n\u001b[1;32m   1802\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File checkpoint/FFnet_Classify_V0-0.meta does not exist."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('checkpoint/FFnet_Classify_V0-0.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./checkpoint/'))\n",
    "# saver.restore(sess, 'checkpoint/FFnet_Classify_')\n",
    "# with tf.Session() as sess:\n",
    "#     new_saver = tf.train.import_meta_graph('my_test_model-1000.meta')\n",
    "#     new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "# import tensorflow as tf\n",
    " \n",
    "# sess=tf.Session()    \n",
    "# #First let's load meta graph and restore weights\n",
    "# saver = tf.train.import_meta_graph('my_test_model-1000.meta')\n",
    "# saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    " \n",
    " \n",
    "# # Now, let's access and create placeholders variables and\n",
    "# # create feed-dict to feed new data\n",
    " \n",
    "# graph = tf.get_default_graph()\n",
    "# w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "# w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "# feed_dict ={w1:13.0,w2:17.0}\n",
    " \n",
    "# #Now, access the op that you want to run. \n",
    "# op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    " \n",
    "# print sess.run(op_to_restore,feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
