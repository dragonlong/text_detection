{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:29:26.331001Z",
     "start_time": "2018-01-17T15:29:25.522327Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import python library\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "# configure and test\n",
    "tf.test.gpu_device_name()\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1, 2,3],[1, 2,3]], dtype = tf.float32)\n",
    "a.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     61
    ]
   },
   "outputs": [],
   "source": [
    "# define the neural network architecture \n",
    "# define needed variables during training \n",
    "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
    "def _variable_summaries(var):\n",
    "    \"\"\"\n",
    "    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    \"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "    \n",
    "class model(object):\n",
    "    # class level attributes, used for \n",
    "    number = 0\n",
    "    def __init__(self, name, input_size, class_num):\n",
    "        model.name = name\n",
    "        w, h, c = input_size\n",
    "        with tf.name_scope('input'):\n",
    "            self.x  = tf.placeholder(tf.float32, shape = [None, w, h, c], name = 'input-x')\n",
    "            self.y  = tf.placeholder(tf.float32, shape = [None, class_num], name ='input-y') \n",
    "    # finally build our model according to our own need\n",
    "    def builder(self):\n",
    "        self.conv1 = self._conv_layer(self.x,     [5, 5, 3, 32], 'conv1')\n",
    "        self.conv2 = self._conv_layer(self.conv1, [3, 3, 32, 64], 'conv2')\n",
    "        self.pool1 = self._max_pool22(self.conv2)\n",
    "        # \n",
    "        self.ffn1  = self._ffnet_module(self.pool1,f = [3, 5], filters = [64, 64, 64, 64] , stage = 1, block = 'a') \n",
    "        self.fc1   = self._fc_layer(tf.reshape(ffnet, [-1, 12*12*128]), 12*12*128, 256, 'fc1' )\n",
    "        self.fc2   = self._fc_layer(self.fc1, 256, 10, 'fc2')\n",
    "        self.y_    = tf.nn.softmax(self.fc2, 2)\n",
    "    # class methods to construct conv-layer, generating summary autotically \n",
    "    # with default stride 1, and relu activations\n",
    "    def _conv_layer(self, X, shape, layer_name, padding='SAME'):\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('W'):\n",
    "                weight = self.get_filters(shape)\n",
    "                _variable_summaries(weight)\n",
    "            with tf.name_scope('b'):\n",
    "                bias = self.get_bias(shape[3])\n",
    "                print(shape[3])\n",
    "                _variable_summaries(bias)\n",
    "            with tf.name_scope('z'):\n",
    "                preactivations = tf.nn.conv2d(X, weight, strides=[1, 1, 1, 1], padding=padding) + bias\n",
    "                _variable_summaries(preactivations)\n",
    "            with tf.name_scope('A'):\n",
    "                activations = tf.nn.relu(preactivations)\n",
    "                _variable_summaries(activations)\n",
    "        return activations\n",
    "    \n",
    "    # ex: prob = tf.constant(0.5, dtype=tf.float32)\n",
    "    def _drop_out(self, X, prob, layer_name):\n",
    "        with tf.name_scope(layer_name+'Dropout'):\n",
    "            return tf.nn.dropout(X, prob)\n",
    "    def _max_pool22(self, X, padding = 'SAME'):\n",
    "        return tf.nn.max_pool(x, strides= [1, 2, 2, 1], ksize=[1, 2, 2, 1], padding= padding)\n",
    "           \n",
    "    def _fc_layer(self, X, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        with tf.name_scope(layer_name):\n",
    "      # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('W'):\n",
    "                weights = self.get_weights([input_dim, output_dim])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('b'):\n",
    "                biases = self.get_bias([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('z'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "                tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "\n",
    "    def _ffnet_module(self, X, f, filters , stage, block):\n",
    "        \"\"\"\n",
    "        Implementation of the figure above, with 3 standard 3*3*64 module for the general module, and one fast-forwarding path\n",
    "        Arguments: \n",
    "        X--      the input tensor with shape (n_H, n_W, n_C)\n",
    "        f--      filter kernel size\n",
    "        filters: number of filters in each layer\n",
    "        stage  : name of stage \n",
    "        block  : string/character, used to name the layers, depending on \n",
    "\n",
    "        Returns:\n",
    "        X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "        \"\"\"\n",
    "        conv_name_base = 'ffnet' + str(stage) + block + 'conv_branch'\n",
    "        bn_name_base   = 'bn' + str(stage) + block + 'bn_branch'\n",
    "        fast_fwd_base  = 'ffnet' + str(stage) + block + 'fast_branch' \n",
    "\n",
    "        # Retrieve filters \n",
    "        F1, F2, F3, F4 = filters\n",
    "        f1, f2     = f\n",
    "        m = X.get_shape().as_list()[3]\n",
    "\n",
    "        # Save the input value\n",
    "        X_fast = X\n",
    "\n",
    "        ####### Main Path ######\n",
    "        X = self._conv_layer(X, [f1, f1, m, F1],  layer_name = conv_name_base + '1', padding = 'VALID')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_1', X)\n",
    "        X = self._conv_layer(X, [f1, f1, F1, F2], layer_name = conv_name_base + '2', padding = 'VALID')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_2', X)\n",
    "        X = self._conv_layer(X, [f1, f1, F2, F3], layer_name = conv_name_base + '3')\n",
    "        ####### fast forward ###### \n",
    "        X_fast = self._conv_layer(X_fast, [f2, f2, m, F4], layer_name = fast_fwd_base + '1')\n",
    "        setattr(self, 'ffnet'+str(stage)+'_4', X_fast)\n",
    "        result = tf.concat([X, X_fast], 3) \n",
    "        print(X_fast)\n",
    "        print(X)\n",
    "        print(result)\n",
    "        ###### Final step: concatation #######\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def get_filters(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return(tf.Variable(initial))\n",
    "    \n",
    "    def get_weights(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return(tf.Variable(initial))\n",
    "    \n",
    "    def get_bias(self, output_dim):\n",
    "        initial = tf.constant(0.1, shape = output_dim)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-914ef91f17ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#module test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mffnet_s1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ffnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mffnet_s1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-db703df13f7e>\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# finally build our model according to our own need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_pool22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-db703df13f7e>\u001b[0m in \u001b[0;36m_conv_layer\u001b[0;34m(self, X, shape, layer_name, padding)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0m_variable_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0m_variable_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-db703df13f7e>\u001b[0m in \u001b[0;36mget_bias\u001b[0;34m(self, output_dim)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnparray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0mshape_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mis_same_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnparray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#module test\n",
    "ffnet_s1 = model('ffnet', [32, 32, 3], 2)\n",
    "ffnet_s1.builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Data import with Dataset\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"train-00000-of-00001\"]\n",
    "train = sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "# validation_filenames = [\"validation-00000-of-00001\"]\n",
    "# val = sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:30:46.647995Z",
     "start_time": "2018-01-17T15:30:46.547942Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_5:0\", shape=(?, 12, 12, 64), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 12, 12, 64), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 12, 12, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "################# start to build up our model !###############\n",
    "x = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "# the first convolutional layer \n",
    "conv_1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "#x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "h_conv1 = tf.nn.relu(conv2d(x, conv_1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# the second convolutional layer+relu+ \n",
    "conv_2 =  weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, conv_2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "ffnet = ffnet_module(h_pool1, f = [3, 5], filters = [64, 64, 64] , stage = 1, block = 'a', s = 1 )\n",
    "# dense connected layer 1\n",
    "W_fc1 = weight_variable([12*12*128, 256])\n",
    "b_fc1 = bias_variable([256])\n",
    "hpool_flat = tf.reshape(ffnet, [-1, 12*12*128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(hpool_flat, W_fc1) + b_fc1)\n",
    "# dense connected layer 2, etc\n",
    "W_fc2 = weight_variable([256, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "# dense connected layer 3, etc\n",
    "W_fc3 = weight_variable([10, 2])\n",
    "b_fc3 = bias_variable([2])\n",
    "# Dropout design\n",
    "prob = tf.placeholder(tf.float32)\n",
    "h_fc1dropout = tf.nn.relu(tf.nn.dropout(h_fc1, prob))\n",
    "h_fc2= tf.matmul(h_fc1dropout, W_fc2) + b_fc2\n",
    "# output channel [node1, node2]\n",
    "y_conv = tf.matmul(h_fc2, W_fc3) + b_fc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# import data in the format of tf.\n",
    "height=tf.constant(32, dtype=tf.int32)\n",
    "width =tf.constant(32, dtype=tf.int32)\n",
    "def read_and_decode(filename, batch_size, num_epochs):\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      # Defaults are not specified since both keys are required.\n",
    "      features={\n",
    "            'image/height': tf.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.FixedLenFeature([], tf.int64),\n",
    "            'image/colorspace': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/channels':  tf.FixedLenFeature([], tf.int64),            \n",
    "            'image/class/label': tf.FixedLenFeature([],tf.int64),\n",
    "            'image/class/text': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/format': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/filename': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value='')\n",
    "      })\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "    # [mnist.IMAGE_PIXELS].\n",
    "#     image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "#     image.set_shape([mnist.IMAGE_PIXELS])\n",
    "    label = features['image/class/label']\n",
    "    image_buffer = features['image/encoded']\n",
    "\n",
    "    # Decode the jpeg\n",
    "    with tf.name_scope('decode_jpeg',[image_buffer], None):\n",
    "        # decode\n",
    "        image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "        \n",
    "        # resize\n",
    "        image = tf.image.resize_images(image, [height, width])\n",
    "    \n",
    "        # and convert to single precision data type\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    # OPTIONAL: Could reshape into a 28x28 image and apply distortions\n",
    "    # here.  Since we are not applying any distortions in this\n",
    "    # example, and the next step expects the image to be flattened\n",
    "    # into a vector, we don't bother.\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    print(image.shape)\n",
    "    #image=tf.reshape(image,[height*width*3])\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(label, tf.int32)  \n",
    "#     images, sparse_labels = tf.train.shuffle_batch(\n",
    "#         [image, label], batch_size=batch_size, num_threads=2,\n",
    "#         capacity=1000 + 3 * batch_size,\n",
    "#         # Ensures a minimum amount of shuffling of examples.\n",
    "#         min_after_dequeue=1000)\n",
    "    return image, label\n",
    "images, labels = read_and_decode(\"train-00000-of-00001\", 1, 10)\n",
    "vimages, vlabels = read_and_decode(\"validation-00000-of-00001\", 1, 10)\n",
    "# training examples \n",
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [images, labels], batch_size=100,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)\n",
    "# and similarly for the validation data \n",
    "vimageBatch, vlabelBatch = tf.train.shuffle_batch(\n",
    "    [vimages, vlabels], batch_size=1,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)\n",
    "# # interactive session allows inteleaving of building and running steps\n",
    "# sess = tf.InteractiveSession()\n",
    "# batch_size =  100\n",
    "# dataset_train = dataset_train.batch(batch_size)\n",
    "# iterator = dataset_train.make_one_shot_iterator()\n",
    "# batch_features, batch_labels = iterator.get_next()\n",
    "sess = tf.InteractiveSession()\n",
    "print(imageBatch.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# explore ways to iomport data: to import data in an iterative style, while aslo assign data and labels dynamically\n",
    "######1. Create Dataset object #######\n",
    "# function for image reading\n",
    "import cv2\n",
    "def read_patch_cv2(filename, label):\n",
    "    image_decoded = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "    print('here I come')\n",
    "    return image_decoded, label\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)#labels are not put here \n",
    "    image = tf.image.decode_image(image_string, channels =  3)\n",
    "    print('Transforming')\n",
    "    return image, label\n",
    "\n",
    "def imgs_input_fn(filenames, labels = None):\n",
    "    # create labels if there is no source\n",
    "    if labels is None:\n",
    "        labels = [0]*len(filenames)\n",
    "    labels = np.array(labels)\n",
    "    # to avoid \n",
    "    if len(labels) == 1:\n",
    "        np.expand_dims(labels, axis = 1)\n",
    "    # convert array/list into tensorflow constants\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "    #abels = tf.cast(labels, tf.float32)\n",
    "    # apply standard tf function mapping images data to dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    return dataset\n",
    "\n",
    "def read_txt(filepath, prefix):\n",
    "    filenames = []\n",
    "    labels= []\n",
    "    # read from text files \n",
    "    with open(filepath, 'r') as f:\n",
    "        num = 0\n",
    "        for line in f:\n",
    "            num +=1 \n",
    "            line = line.strip()\n",
    "            if num < 1:\n",
    "                print('%s'  %(line[:-2]) )\n",
    "                print(line[-1])\n",
    "            filenames.append(prefix+line[:-2])\n",
    "            labels.append(line[-1])\n",
    "    print(num)\n",
    "    return filenames, labels\n",
    "    \n",
    "#print(filenames)\n",
    "filepath_train = '/home/dragonx/Documents/text_detect/dataC/COCO-Text-Patch/train.txt'\n",
    "filepath_val = '/home/dragonx/Documents/text_detect/dataA/COCO-Text-Patch/val.txt'\n",
    "filenames_train, labels_train = read_txt(filepath_train, './dataC/COCO-Text-Patch/images/')\n",
    "filenames_val, labels_val = read_txt(filepath_val, './dataA/COCO-Text-Patch/images/')\n",
    "dataset_train = imgs_input_fn(filenames_train, labels_train)\n",
    "dataset_val   = imgs_input_fn(filenames_val, labels_val)\n",
    "######2. batch_read() ####### \n",
    "#either from file directory, or by the txt file with filenames  \n",
    "\n",
    "######3. Create tfRecord Object #######\n",
    "\n",
    "#(recommended)\n",
    "######4. Read from npy format(one by one or in total)  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Data import with Dataset\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"train-00000-of-00001\"]\n",
    "train = sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "# validation_filenames = [\"validation-00000-of-00001\"]\n",
    "# val = sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dragonx/Documents/text_detect\r\n"
     ]
    }
   ],
   "source": [
    "# make sure the directory is correct\n",
    "!pwd\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108012\n",
      "122324\n",
      "['./dataC/COCO-Text-Patch/images/txt/txt_615639.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_660131.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_235518.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_734094.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_336638.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_374506.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_127162.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_713484.jpg'\n",
      " './dataC/COCO-Text-Patch/images/txt/txt_43367.jpg']\n"
     ]
    }
   ],
   "source": [
    "# self defined data import as numpy array\n",
    "import os\n",
    "import cv2\n",
    "train_dir = './dataC/COCO-Text-Patch/images/'\n",
    "val_dir =   './dataA/COCO-Text-Patch/images/'\n",
    "classes = ['txt', 'nontxt']\n",
    "fname_tr   = []\n",
    "labels_tr  = []\n",
    "fname_val  = []\n",
    "labels_val = []\n",
    "for i, c in enumerate(classes):\n",
    "    buffer = os.listdir(train_dir+c)\n",
    "    buffer = [train_dir+c+'/'+x for x in buffer]\n",
    "    fname_tr.extend(buffer[:])\n",
    "    labels_tr.extend([i]*len(buffer))\n",
    "    # the same for val\n",
    "    buffer = os.listdir(val_dir+c)\n",
    "    buffer = [val_dir+c+'/'+x for x in buffer]\n",
    "    fname_val.extend(buffer[:])\n",
    "    labels_val.extend([i]*len(buffer))\n",
    "\n",
    "print(len(fname_tr))\n",
    "print(len(fname_val))\n",
    "# print(fname_tr[0:100])\n",
    "# check if numbers are correct\n",
    "assert len(fname_tr) == len(labels_tr)\n",
    "index_tr = np.arange(0, len(labels_tr))  \n",
    "index_val= np.arange(0, len(labels_val))  \n",
    "def imread_batch(filenames, labels):\n",
    "    train_data = np.zeros([len(filenames), 32, 32, 3])\n",
    "    for i, f in enumerate(filenames):\n",
    "        #print(f)\n",
    "        buffer = cv2.imread(f)\n",
    "        train_data[i, :,:,:] = buffer[:,:,:]\n",
    "        #print(buffer.shape)\n",
    "    labels = one_hot_matrix(labels, 2).T\n",
    "    \n",
    "    return train_data, labels\n",
    "\n",
    "#data_tr, labels_tr = imread_batch(fname_tr[0:100], labels_tr[0:100])\n",
    "#data_val,labels_val = imread_batch(fname_val[0:100], labels_val[0:100])\n",
    "# data_tr, labels_tr = imread_batch(fname_tr(index_tr[0:100]) , labels_tr[0:100])\n",
    "print(np.take(fname_tr, index_tr[1:10], axis = 0))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 32, 32, 3)\n",
      "(99, 2)\n"
     ]
    }
   ],
   "source": [
    "# input data produce\n",
    "data_batch, labels_batch = imread_batch(fname_tr[1:100], labels_tr[1:100]); \n",
    "print(data_batch.shape)\n",
    "print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "# OOD programming\n",
    "def _create_summaries(loss, accuracy):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        tf.summary.scalar(\"histogram loss\", loss)\n",
    "        # merge together\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    return summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n",
      "epoch 0\n",
      "load & training time on 100 images is 0.634101\n",
      "load & training time on 100 images is 0.546075\n",
      "load & training time on 100 images is 0.986707\n",
      "load & training time on 100 images is 0.548373\n",
      "current accuracy on 100 images is 0.660000\n",
      "epoch 1\n",
      "load & training time on 100 images is 0.566722\n",
      "load & training time on 100 images is 1.513245\n",
      "load & training time on 100 images is 1.101350\n",
      "load & training time on 100 images is 1.203141\n",
      "current accuracy on 100 images is 0.680000\n",
      "epoch 2\n",
      "load & training time on 100 images is 0.561458\n",
      "load & training time on 100 images is 1.285371\n",
      "load & training time on 100 images is 0.702812\n",
      "load & training time on 100 images is 0.713212\n",
      "current accuracy on 100 images is 0.710000\n",
      "epoch 3\n",
      "load & training time on 100 images is 0.945154\n",
      "load & training time on 100 images is 0.967590\n",
      "load & training time on 100 images is 0.705250\n",
      "load & training time on 100 images is 0.666004\n",
      "current accuracy on 100 images is 0.720000\n",
      "epoch 4\n",
      "load & training time on 100 images is 0.812146\n",
      "load & training time on 100 images is 0.792162\n",
      "load & training time on 100 images is 0.849050\n",
      "load & training time on 100 images is 0.972744\n",
      "current accuracy on 100 images is 0.790000\n",
      "epoch 5\n",
      "load & training time on 100 images is 1.169891\n",
      "load & training time on 100 images is 0.911009\n",
      "load & training time on 100 images is 0.929845\n",
      "load & training time on 100 images is 0.807305\n",
      "current accuracy on 100 images is 0.750000\n",
      "epoch 6\n",
      "load & training time on 100 images is 0.973673\n",
      "load & training time on 100 images is 1.625612\n",
      "load & training time on 100 images is 1.264197\n",
      "load & training time on 100 images is 1.655228\n",
      "current accuracy on 100 images is 0.780000\n",
      "epoch 7\n",
      "load & training time on 100 images is 1.057961\n",
      "load & training time on 100 images is 1.094862\n",
      "load & training time on 100 images is 1.099407\n",
      "load & training time on 100 images is 1.680179\n",
      "current accuracy on 100 images is 0.690000\n",
      "epoch 8\n",
      "load & training time on 100 images is 1.290815\n",
      "load & training time on 100 images is 0.978182\n",
      "load & training time on 100 images is 1.814565\n",
      "load & training time on 100 images is 1.206580\n",
      "current accuracy on 100 images is 0.760000\n",
      "epoch 9\n",
      "load & training time on 100 images is 1.235185\n",
      "load & training time on 100 images is 1.241413\n",
      "load & training time on 100 images is 1.066995\n",
      "load & training time on 100 images is 2.448991\n",
      "current accuracy on 100 images is 0.710000\n",
      "epoch 10\n",
      "load & training time on 100 images is 1.311141\n",
      "load & training time on 100 images is 1.332684\n",
      "load & training time on 100 images is 1.151464\n",
      "load & training time on 100 images is 2.059020\n",
      "current accuracy on 100 images is 0.750000\n",
      "epoch 11\n",
      "load & training time on 100 images is 1.408660\n",
      "load & training time on 100 images is 1.743840\n",
      "load & training time on 100 images is 1.313951\n",
      "load & training time on 100 images is 1.243536\n",
      "current accuracy on 100 images is 0.750000\n",
      "epoch 12\n",
      "load & training time on 100 images is 1.379351\n",
      "load & training time on 100 images is 1.511535\n",
      "load & training time on 100 images is 1.097638\n",
      "load & training time on 100 images is 1.599357\n",
      "current accuracy on 100 images is 0.790000\n",
      "epoch 13\n",
      "load & training time on 100 images is 1.743360\n",
      "load & training time on 100 images is 2.854324\n",
      "load & training time on 100 images is 1.580177\n",
      "load & training time on 100 images is 2.003370\n",
      "current accuracy on 100 images is 0.840000\n",
      "epoch 14\n",
      "load & training time on 100 images is 1.611384\n",
      "load & training time on 100 images is 2.253484\n",
      "load & training time on 100 images is 1.439807\n",
      "load & training time on 100 images is 1.820842\n",
      "current accuracy on 100 images is 0.820000\n",
      "epoch 15\n",
      "load & training time on 100 images is 3.008524\n",
      "load & training time on 100 images is 3.249604\n",
      "load & training time on 100 images is 2.490141\n",
      "load & training time on 100 images is 1.752108\n",
      "current accuracy on 100 images is 0.800000\n",
      "epoch 16\n",
      "load & training time on 100 images is 1.718354\n",
      "load & training time on 100 images is 2.346522\n",
      "load & training time on 100 images is 1.825546\n",
      "load & training time on 100 images is 1.558212\n",
      "current accuracy on 100 images is 0.730000\n",
      "epoch 17\n",
      "load & training time on 100 images is 1.573705\n",
      "load & training time on 100 images is 1.888494\n",
      "load & training time on 100 images is 1.549479\n",
      "load & training time on 100 images is 1.931234\n",
      "current accuracy on 100 images is 0.830000\n",
      "epoch 18\n",
      "load & training time on 100 images is 1.947307\n",
      "load & training time on 100 images is 1.654334\n",
      "load & training time on 100 images is 1.707913\n",
      "load & training time on 100 images is 3.057350\n",
      "current accuracy on 100 images is 0.850000\n",
      "epoch 19\n",
      "load & training time on 100 images is 3.856287\n",
      "load & training time on 100 images is 1.725506\n",
      "load & training time on 100 images is 1.842194\n",
      "load & training time on 100 images is 2.767054\n",
      "current accuracy on 100 images is 0.810000\n"
     ]
    }
   ],
   "source": [
    "############ for training detail ##############\n",
    "# x, y_: input labels\n",
    "# y_conv: convolutional layers output\n",
    "from random  import shuffle\n",
    "import time\n",
    "# loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y_conv))\n",
    "# optimizer and learning rate\n",
    "train_step   = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# how to decide the prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# train process \n",
    "batch_size    = 100\n",
    "epoch_num     = 20\n",
    "train_record  = []\n",
    "global_step   = tf.Variable(10, dtype=tf.int32, trainable=False, name = 'global_step')\n",
    "check_summary = _create_summaries(cross_entropy, accuracy)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('./checkpoint/check_summary', sess.graph)\n",
    "    # num_tr = np.floor(len(labels_tr)/batch_size)\n",
    "    num_val = np.floor(len(labels_val)/batch_size)\n",
    "    num_tr = 200\n",
    "    for i in range(epoch_num):\n",
    "        start_time = time.time()\n",
    "        print('epoch %d'%(i) )\n",
    "        # random batch in every iteration\n",
    "        shuffle(index_tr)\n",
    "        for j in range(np.int64(num_tr)):\n",
    "            # data preparation\n",
    "            sub_time = time.time()\n",
    "            filenames_batch = np.take(fname_tr, index_tr[batch_size*(j) :min(batch_size*(j+1), len(labels_tr))], axis = 0)\n",
    "            labels_batch    = np.take(labels_tr, index_tr[batch_size*(j):min(batch_size*(j+1),len(labels_tr))] ,axis = 0)\n",
    "            data_batch, labels_batch = imread_batch(filenames_batch, labels_batch);#print(data_batch.shape)\n",
    "            sub_time1 = time.time()\n",
    "            #print('loading time on %d images is %f'%(batch_size, sub_time1 -sub_time))   \n",
    "            # train the network from scratch using GPU\n",
    "            train_step.run(feed_dict = {x: data_batch, y_ :labels_batch, prob:0.5})\n",
    "            summary = sess.run(check_summary, feed_dict = {x: data_batch, y_ :labels_batch, prob:0.5})\n",
    "            summary_writer.add_summary(summary, i*num_tr+j)\n",
    "            epoch_check = 5\n",
    "#           if (j+1)%5 == 0:\n",
    "#               saver.save(sess, 'checkpoint/FFnet_Classify_V0', global_step = global_step )\n",
    "            if j%50 == 0:\n",
    "                print('load & training time on %d images is %f'%(batch_size, time.time()-sub_time))\n",
    "        # model evaluation with val dataset\n",
    "        train_accuracy = accuracy.eval(feed_dict = {x: data_batch, y_ : labels_batch, prob: 1})\n",
    "        train_record.append(train_accuracy)\n",
    "        print('current accuracy on %d images is %f'%(batch_size, train_accuracy))\n",
    "        # training time evaluation with timer\n",
    "        # provide checkpoints to save models sequentially\n",
    "        epoch_check = 5\n",
    "        if (i+1)%5 == 0:\n",
    "            saver.save(sess, 'checkpoint/FFnet_Classify_V0', global_step = tf.convert_to_tensor(epoch_check*num_tr))\n",
    "            \n",
    "            \n",
    "        # data visualization & model summary  \n",
    "        \n",
    "#         if i % 10 == 0:\n",
    "#             train_accuracy = accuracy.eval(feed_dict = {x: , y_ : labelBatch, prob: 1})\n",
    "#             print('step %d, training accuracy %g' %(i, train_accuracy))\n",
    "#         filenames_batch = np.take(fname_tr, index_tr[batch_size*(j):batch_size*(j+1)], axis = 0)\n",
    "#         labels_batch    = np.take(labels_tr, index_tr[batch_size*(i):batch_size*(i+1)], axis = 0)\n",
    "#         data_batch, labels_batch = imread_batch(filenames_batch, labels_batch)\n",
    "#         print('test accuracy %g' %accuracy.eval(feed_dict = {x: vimageBatch, y_: vimageBatch, prob : 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.66000003,\n",
       " 0.68000001,\n",
       " 0.70999998,\n",
       " 0.72000003,\n",
       " 0.79000002,\n",
       " 0.75,\n",
       " 0.77999997,\n",
       " 0.69,\n",
       " 0.75999999,\n",
       " 0.70999998,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.79000002,\n",
       " 0.83999997,\n",
       " 0.81999999,\n",
       " 0.80000001,\n",
       " 0.73000002,\n",
       " 0.82999998,\n",
       " 0.85000002,\n",
       " 0.81]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File checkpoint/FFnet_Classify_V0-0.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-569fac543aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint/FFnet_Classify_V0-0.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoint/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#saver.restore(sess, 'checkpoint/FFnet_Classify_')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m                      \"execution is enabled.\")\n\u001b[1;32m   1802\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File checkpoint/FFnet_Classify_V0-0.meta does not exist."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('checkpoint/FFnet_Classify_V0-0.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./checkpoint/'))\n",
    "#saver.restore(sess, 'checkpoint/FFnet_Classify_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dragonx/Documents/text_detect\r\n"
     ]
    }
   ],
   "source": [
    "# not ood style of summary\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
